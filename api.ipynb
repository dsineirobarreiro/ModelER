{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dsineirobarreiro/ModelER/blob/main/api.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Environment configuration\n",
        "\n",
        "With these commands, the environment is configured to run on GPU. Uncomment cell 3 to use the CPU"
      ],
      "metadata": {
        "id": "67UsV7OY7ySs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on -DCMAKE_CUDA_ARCHITECTURES=61\" FORCE_CMAKE=1 pip install llama-cpp-python==0.2.62 --force-reinstall --upgrade --no-cache-dir --verbose --no-build-isolation"
      ],
      "metadata": {
        "id": "wkX2JYgcECMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "yWG5wTDq67bz"
      },
      "outputs": [],
      "source": [
        "!pip install fastapi pydantic-settings uvicorn langchain-experimental jsonref python-multipart ngrok"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install llama-cpp-python"
      ],
      "metadata": {
        "id": "vwGrEhR789UM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Download the model\n",
        "It is necesasary to log in with Hugging Face to download the model"
      ],
      "metadata": {
        "id": "WZ5Z-9od7_Qj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KZI7rZ1EHqGv"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli download TheBloke/Llama-2-7B-Chat-GGUF llama-2-7b-chat.Q5_K_M.gguf --local-dir ./json/ --local-dir-use-symlinks False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model configuration"
      ],
      "metadata": {
        "id": "gTNeGlAt8GQ7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "WjYNNDkoGIdh"
      },
      "outputs": [],
      "source": [
        "from langchain_community.llms.llamacpp import LlamaCpp\n",
        "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Optional\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain.chains.llm import LLMChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_experimental.chat_models import Llama2Chat\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    MessagesPlaceholder,\n",
        ")\n",
        "from langchain_core.messages import SystemMessage\n",
        "from langchain.memory import ChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from jsonref import replace_refs\n",
        "import json\n",
        "from llama_cpp import LlamaGrammar\n",
        "from typing import Literal\n",
        "\n",
        "model_path = '/content/json/llama-2-7b-chat.Q5_K_M.gguf'\n",
        "\n",
        "# Callbacks support token-wise streaming\n",
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
        "\n",
        "class Attribute(BaseModel):\n",
        "    name: str = Field(description='attribute name')\n",
        "    type: str = Field(description='type of the atrtibute')\n",
        "\n",
        "# Define your desired data structure.\n",
        "class Entity(BaseModel):\n",
        "    name: str = Field(description='entity name from the case scenario')\n",
        "    attributes: Optional[List[Attribute]] = Field(description=\"colection of attributes from the entity\")\n",
        "\n",
        "class Relation(BaseModel):\n",
        "    name: str = Field(description='name of the relation')\n",
        "    source: str = Field(description='source entity of the relation')\n",
        "    cardinality_of_source: Literal['Zero or One', 'Exactly One', 'Zero or Many', 'One or Many']\n",
        "    target: str = Field(description='target entity of the relation')\n",
        "    cardinality_of_target: Literal['Zero or One', 'Exactly One', 'Zero or Many', 'One or Many']\n",
        "\n",
        "\n",
        "class Main(BaseModel):\n",
        "    entities: List[Entity] = Field(description='Entity from the case scenario')\n",
        "    relations: List[Relation] = Field(description='colection of relations from the entities')\n",
        "\n",
        "def json_schema_with_inlining(model):\n",
        "    replaced = replace_refs(model.schema(), proxies=False)\n",
        "    if \"$defs\" in replaced:\n",
        "        del replaced[\"$defs\"]\n",
        "    return json.dumps(replaced)\n",
        "\n",
        "grammar = LlamaGrammar.from_json_schema(json_schema_with_inlining(Main))\n",
        "\n",
        "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
        "You are an expert in data modeling for creating databases. Imagine a client is talking to you in order to create a database for their business.\n",
        "They will present you a scenario where your goal will be to extract entities, their attributes and their relations so the first step for creating\n",
        "a database can be achieve. As it is client who asks for this modeling, you must be very accurate and effective in the extraction so work step by step.\n",
        "Source and target of the relations must be names of entities and every entity must be a participant of some relationship.\n",
        "\"\"\"\n",
        "\n",
        "llm = LlamaCpp(\n",
        "    model_path=model_path,\n",
        "    n_ctx=4096,\n",
        "    n_gpu_layers=-1,\n",
        "    max_tokens=-1,\n",
        "    callback_manager=callback_manager,\n",
        "    verbose=True,\n",
        "    streaming=True,\n",
        "    grammar=grammar,\n",
        "    temperature=0.2,\n",
        ")\n",
        "\n",
        "model = Llama2Chat(llm=llm)\n",
        "\n",
        "template_messages = [\n",
        "    SystemMessage(content=DEFAULT_SYSTEM_PROMPT),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    HumanMessagePromptTemplate.from_template(\"{input}\"),\n",
        "]\n",
        "prompt_template = ChatPromptTemplate.from_messages(template_messages)\n",
        "\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "chain = LLMChain(llm=model, prompt=prompt_template, memory=memory)\n",
        "\n",
        "chains = {}\n",
        "chains['llama2'] = {}\n",
        "chains['generate'] = chain"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# API configuration"
      ],
      "metadata": {
        "id": "4H6cs9kC8Jr7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3WhrJ5nEoLJ"
      },
      "outputs": [],
      "source": [
        "from fastapi import FastAPI, Form\n",
        "from fastapi.responses import StreamingResponse\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=['*'],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=['*'],\n",
        "    allow_headers=['*'],\n",
        ")\n",
        "\n",
        "async def gen(model, action, prompt: str):\n",
        "    async for event in chain.astream_events(\n",
        "        {\"input\": prompt},\n",
        "        version='v1'\n",
        "    ):\n",
        "        if event['event'] == 'on_chat_model_stream':\n",
        "            yield event['data']['chunk']\n",
        "\n",
        "@app.post('/{model}/{action}/')\n",
        "async def generate(action, prompt: str = Form()):\n",
        "    return StreamingResponse(gen(model, action, prompt), media_type='text/event-stream')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ngrok tunnel"
      ],
      "metadata": {
        "id": "ASnQcx648Na6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3M-bbSH9D1PH"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import ngrok\n",
        "import uvicorn\n",
        "import nest_asyncio\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "async def setup():\n",
        "        ngrok.set_auth_token(userdata.get(\"NGROK\"))\n",
        "        ngrok.forward(\n",
        "            8000,\n",
        "            authtoken_from_env=True,\n",
        "            #Change the following parameter with your Ngrok domain\n",
        "            domain=\"bright-akita-pleasantly.ngrok-free.app\"\n",
        "        )\n",
        "\n",
        "nest_asyncio.apply()\n",
        "asyncio.run(setup())\n",
        "uvicorn.run(app=app)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPOobkL6AmBngOanL7XgNGG",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}