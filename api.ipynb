{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dsineirobarreiro/ModelER/blob/main/api.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjsMLe2VJwm0",
        "outputId": "8448a97d-1c65-4c8e-a57d-00935d6dd1d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ngrok\n",
            "  Downloading ngrok-1.4.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Downloading ngrok-1.4.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ngrok\n",
            "Successfully installed ngrok-1.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install ngrok"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on -DCMAKE_CUDA_ARCHITECTURES=61\" FORCE_CMAKE=1 pip install llama-cpp-python==0.2.62 --force-reinstall --upgrade --no-cache-dir --verbose --no-build-isolation"
      ],
      "metadata": {
        "id": "wkX2JYgcECMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yWG5wTDq67bz",
        "outputId": "8df18ed2-e57b-4dfb-c0b0-3d03e7684ed3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.90.tar.gz (63.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fastapi\n",
            "  Downloading fastapi-0.115.0-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting pydantic-settings\n",
            "  Downloading pydantic_settings-2.5.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.30.6-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting langchain-experimental\n",
            "  Downloading langchain_experimental-0.3.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting jsonref\n",
            "  Downloading jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting python-multipart\n",
            "  Downloading python_multipart-0.0.10-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting deflate\n",
            "  Downloading deflate-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.26.4)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.4)\n",
            "Collecting starlette<0.39.0,>=0.37.2 (from fastapi)\n",
            "  Downloading starlette-0.38.6-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (2.9.2)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting langchain-community<0.4.0,>=0.3.0 (from langchain-experimental)\n",
            "  Downloading langchain_community-0.3.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.0 (from langchain-experimental)\n",
            "  Downloading langchain_core-0.3.5-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.10.5)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community<0.4.0,>=0.3.0->langchain-experimental)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting langchain<0.4.0,>=0.3.0 (from langchain-community<0.4.0,>=0.3.0->langchain-experimental)\n",
            "  Downloading langchain-0.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.112 (from langchain-community<0.4.0,>=0.3.0->langchain-experimental)\n",
            "  Downloading langsmith-0.1.126-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.32.3)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain-community<0.4.0,>=0.3.0->langchain-experimental)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.0->langchain-experimental)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain-experimental) (24.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.23.4)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.39.0,>=0.37.2->fastapi) (3.7.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (4.0.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.39.0,>=0.37.2->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.39.0,>=0.37.2->fastapi) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.39.0,>=0.37.2->fastapi) (1.2.2)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental)\n",
            "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.0->langchain-experimental)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain<0.4.0,>=0.3.0->langchain-community<0.4.0,>=0.3.0->langchain-experimental)\n",
            "  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.112->langchain-community<0.4.0,>=0.3.0->langchain-experimental)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.112->langchain-community<0.4.0,>=0.3.0->langchain-experimental)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.1.0)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-community<0.4.0,>=0.3.0->langchain-experimental)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading fastapi-0.115.0-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.5.2-py3-none-any.whl (26 kB)\n",
            "Downloading uvicorn-0.30.6-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_experimental-0.3.0-py3-none-any.whl (206 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.9/206.9 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
            "Downloading python_multipart-0.0.10-py3-none-any.whl (22 kB)\n",
            "Downloading deflate-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.0-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.5-py3-none-any.whl (399 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.9/399.9 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading starlette-0.38.6-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading langchain-0.3.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langsmith-0.1.126-py3-none-any.whl (290 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n",
            "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.90-cp310-cp310-linux_x86_64.whl size=3398627 sha256=b9533dc8aa904fec4dc115bb755f59d05d61b30decf8abca28b5760dd54fcc5b\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/67/02/f950031435db4a5a02e6269f6adb6703bf1631c3616380f3c6\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: tenacity, python-multipart, python-dotenv, orjson, mypy-extensions, marshmallow, jsonref, jsonpointer, h11, diskcache, deflate, uvicorn, typing-inspect, starlette, llama-cpp-python, jsonpatch, httpcore, pydantic-settings, httpx, fastapi, dataclasses-json, langsmith, langchain-core, langchain-text-splitters, langchain, langchain-community, langchain-experimental\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed dataclasses-json-0.6.7 deflate-0.7.0 diskcache-5.6.3 fastapi-0.115.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jsonpatch-1.33 jsonpointer-3.0.0 jsonref-1.1.0 langchain-0.3.0 langchain-community-0.3.0 langchain-core-0.3.5 langchain-experimental-0.3.0 langchain-text-splitters-0.3.0 langsmith-0.1.126 llama-cpp-python-0.2.90 marshmallow-3.22.0 mypy-extensions-1.0.0 orjson-3.10.7 pydantic-settings-2.5.2 python-dotenv-1.0.1 python-multipart-0.0.10 starlette-0.38.6 tenacity-8.5.0 typing-inspect-0.9.0 uvicorn-0.30.6\n"
          ]
        }
      ],
      "source": [
        "!pip install fastapi pydantic-settings uvicorn langchain-experimental jsonref python-multipart deflate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "KZI7rZ1EHqGv",
        "outputId": "6ca095a4-150f-4fb2-c18c-921c7596dc32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/commands/download.py:132: FutureWarning: Ignoring --local-dir-use-symlinks. Downloading to a local directory does not use symlinks anymore.\n",
            "  warnings.warn(\n",
            "Downloading 'llama-2-7b-chat.Q5_K_M.gguf' to 'json/.cache/huggingface/download/llama-2-7b-chat.Q5_K_M.gguf.e0b99920cf47b94c78d2fb06a1eceb9ed795176dfa3f7feac64629f1b52b997f.incomplete'\n",
            "llama-2-7b-chat.Q5_K_M.gguf: 100% 4.78G/4.78G [04:20<00:00, 18.4MB/s]\n",
            "Download complete. Moving file to json/llama-2-7b-chat.Q5_K_M.gguf\n",
            "json/llama-2-7b-chat.Q5_K_M.gguf\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli download TheBloke/Llama-2-7B-Chat-GGUF llama-2-7b-chat.Q5_K_M.gguf --local-dir ./json/ --local-dir-use-symlinks False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WjYNNDkoGIdh",
        "outputId": "ba8f74b5-2733-4ad1-c849-9310f01d5faf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /content/json/llama-2-7b-chat.Q5_K_M.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 17\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char ::= [^\"\\] | [\\] char_1 \n",
            "char_1 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
            "entities ::= [[] space entities_8 []] space \n",
            "space ::= space_38 \n",
            "entities_4 ::= entities-item entities_7 \n",
            "entities-item ::= [{] space entities-item-name-kv [,] space entities-item-attributes-kv [}] space \n",
            "entities_6 ::= [,] space entities-item \n",
            "entities_7 ::= entities_6 entities_7 | \n",
            "entities_8 ::= entities_4 | \n",
            "entities-item-name-kv ::= [\"] [n] [a] [m] [e] [\"] space [:] space string \n",
            "entities-item-attributes-kv ::= [\"] [a] [t] [t] [r] [i] [b] [u] [t] [e] [s] [\"] space [:] space entities-item-attributes \n",
            "entities-item-attributes ::= entities-item-attributes-0 | null \n",
            "entities-item-attributes-0 ::= [[] space entities-item-attributes-0_18 []] space \n",
            "null ::= [n] [u] [l] [l] space \n",
            "entities-item-attributes-0_14 ::= entities-item-attributes-0-item entities-item-attributes-0_17 \n",
            "entities-item-attributes-0-item ::= [{] space entities-item-attributes-0-item-name-kv [,] space entities-item-attributes-0-item-type-kv [}] space \n",
            "entities-item-attributes-0_16 ::= [,] space entities-item-attributes-0-item \n",
            "entities-item-attributes-0_17 ::= entities-item-attributes-0_16 entities-item-attributes-0_17 | \n",
            "entities-item-attributes-0_18 ::= entities-item-attributes-0_14 | \n",
            "entities-item-attributes-0-item-name-kv ::= [\"] [n] [a] [m] [e] [\"] space [:] space string \n",
            "entities-item-attributes-0-item-type-kv ::= [\"] [t] [y] [p] [e] [\"] space [:] space string \n",
            "string ::= [\"] string_39 [\"] space \n",
            "entities-kv ::= [\"] [e] [n] [t] [i] [t] [i] [e] [s] [\"] space [:] space entities \n",
            "relations ::= [[] space relations_28 []] space \n",
            "relations_24 ::= relations-item relations_27 \n",
            "relations-item ::= [{] space relations-item-name-kv [,] space relations-item-source-kv [,] space relations-item-cardinality-of-source-kv [,] space relations-item-target-kv [,] space relations-item-cardinality-of-target-kv [}] space \n",
            "relations_26 ::= [,] space relations-item \n",
            "relations_27 ::= relations_26 relations_27 | \n",
            "relations_28 ::= relations_24 | \n",
            "relations-item-name-kv ::= [\"] [n] [a] [m] [e] [\"] space [:] space string \n",
            "relations-item-source-kv ::= [\"] [s] [o] [u] [r] [c] [e] [\"] space [:] space string \n",
            "relations-item-cardinality-of-source-kv ::= [\"] [c] [a] [r] [d] [i] [n] [a] [l] [i] [t] [y] [_] [o] [f] [_] [s] [o] [u] [r] [c] [e] [\"] space [:] space relations-item-cardinality-of-source \n",
            "relations-item-target-kv ::= [\"] [t] [a] [r] [g] [e] [t] [\"] space [:] space string \n",
            "relations-item-cardinality-of-target-kv ::= [\"] [c] [a] [r] [d] [i] [n] [a] [l] [i] [t] [y] [_] [o] [f] [_] [t] [a] [r] [g] [e] [t] [\"] space [:] space relations-item-cardinality-of-target \n",
            "relations-item-cardinality-of-source ::= [\"] [Z] [e] [r] [o] [ ] [o] [r] [ ] [O] [n] [e] [\"] | [\"] [E] [x] [a] [c] [t] [l] [y] [ ] [O] [n] [e] [\"] | [\"] [Z] [e] [r] [o] [ ] [o] [r] [ ] [M] [a] [n] [y] [\"] | [\"] [O] [n] [e] [ ] [o] [r] [ ] [M] [a] [n] [y] [\"] \n",
            "relations-item-cardinality-of-target ::= [\"] [Z] [e] [r] [o] [ ] [o] [r] [ ] [O] [n] [e] [\"] | [\"] [E] [x] [a] [c] [t] [l] [y] [ ] [O] [n] [e] [\"] | [\"] [Z] [e] [r] [o] [ ] [o] [r] [ ] [M] [a] [n] [y] [\"] | [\"] [O] [n] [e] [ ] [o] [r] [ ] [M] [a] [n] [y] [\"] \n",
            "relations-kv ::= [\"] [r] [e] [l] [a] [t] [i] [o] [n] [s] [\"] space [:] space relations \n",
            "root ::= [{] space entities-kv [,] space relations-kv [}] space \n",
            "space_38 ::= [ ] | \n",
            "string_39 ::= char string_39 | \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q5_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens cache size = 3\n",
            "llm_load_vocab: token to piece cache size = 0.1684 MB\n",
            "llm_load_print_meta: format           = GGUF V2\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 4096\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
            "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 11008\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
            "llm_load_print_meta: model params     = 6.74 B\n",
            "llm_load_print_meta: model size       = 4.45 GiB (5.68 BPW) \n",
            "llm_load_print_meta: general.name     = LLaMA v2\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_print_meta: max token length = 48\n",
            "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
            "llm_load_tensors:        CPU buffer size =  4560.87 MiB\n",
            "..................................................................................................\n",
            "llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32\n",
            "llama_new_context_with_model: n_ctx      = 4096\n",
            "llama_new_context_with_model: n_batch    = 32\n",
            "llama_new_context_with_model: n_ubatch   = 32\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =  2048.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =    18.50 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '17'}\n",
            "Using fallback chat format: llama-2\n",
            "<ipython-input-2-e4d63cbc7801>:86: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  chain = LLMChain(llm=model, prompt=prompt_template, memory=memory)\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.llms.llamacpp import LlamaCpp\n",
        "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Optional\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain.chains.llm import LLMChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_experimental.chat_models import Llama2Chat\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    MessagesPlaceholder,\n",
        ")\n",
        "from langchain_core.messages import SystemMessage\n",
        "from langchain.memory import ChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from jsonref import replace_refs\n",
        "import json\n",
        "from llama_cpp import LlamaGrammar\n",
        "from typing import Literal\n",
        "\n",
        "model_path = '/content/json/llama-2-7b-chat.Q5_K_M.gguf'\n",
        "\n",
        "# Callbacks support token-wise streaming\n",
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
        "\n",
        "class Attribute(BaseModel):\n",
        "    name: str = Field(description='attribute name')\n",
        "    type: str = Field(description='type of the atrtibute')\n",
        "\n",
        "# Define your desired data structure.\n",
        "class Entity(BaseModel):\n",
        "    name: str = Field(description='entity name from the case scenario')\n",
        "    attributes: Optional[List[Attribute]] = Field(description=\"colection of attributes from the entity\")\n",
        "\n",
        "class Relation(BaseModel):\n",
        "    name: str = Field(description='name of the relation')\n",
        "    source: str = Field(description='source entity of the relation')\n",
        "    cardinality_of_source: Literal['Zero or One', 'Exactly One', 'Zero or Many', 'One or Many']\n",
        "    target: str = Field(description='target entity of the relation')\n",
        "    cardinality_of_target: Literal['Zero or One', 'Exactly One', 'Zero or Many', 'One or Many']\n",
        "\n",
        "\n",
        "class Main(BaseModel):\n",
        "    entities: List[Entity] = Field(description='Entity from the case scenario')\n",
        "    relations: List[Relation] = Field(description='colection of relations from the entities')\n",
        "\n",
        "def json_schema_with_inlining(model):\n",
        "    replaced = replace_refs(model.schema(), proxies=False)\n",
        "    if \"$defs\" in replaced:\n",
        "        del replaced[\"$defs\"]\n",
        "    return json.dumps(replaced)\n",
        "\n",
        "grammar = LlamaGrammar.from_json_schema(json_schema_with_inlining(Main))\n",
        "\n",
        "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
        "You are an expert in data modeling for creating databases. Imagine a client is talking to you in order to create a database for their business.\n",
        "They will present you a scenario where your goal will be to extract entities, their attributes and their relations so the first step for creating\n",
        "a database can be achieve. As it is client who asks for this modeling, you must be very accurate and effective in the extraction so work step by step.\n",
        "Source and target of the relations must be names of entities and every entity must be a participant of some relationship.\n",
        "\"\"\"\n",
        "\n",
        "llm = LlamaCpp(\n",
        "    model_path=model_path,\n",
        "    n_ctx=4096,\n",
        "    n_gpu_layers=-1,\n",
        "    max_tokens=-1,\n",
        "    callback_manager=callback_manager,\n",
        "    verbose=True,\n",
        "    streaming=True,\n",
        "    grammar=grammar,\n",
        "    temperature=0.2,\n",
        ")\n",
        "\n",
        "model = Llama2Chat(llm=llm)\n",
        "\n",
        "template_messages = [\n",
        "    SystemMessage(content=DEFAULT_SYSTEM_PROMPT),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    HumanMessagePromptTemplate.from_template(\"{input}\"),\n",
        "]\n",
        "prompt_template = ChatPromptTemplate.from_messages(template_messages)\n",
        "\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "chain = LLMChain(llm=model, prompt=prompt_template, memory=memory)\n",
        "\n",
        "chains = {}\n",
        "chains['llama2'] = {}\n",
        "chains['generate'] = chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3WhrJ5nEoLJ"
      },
      "outputs": [],
      "source": [
        "from fastapi import FastAPI, Form\n",
        "from fastapi.responses import StreamingResponse\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=['*'],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=['*'],\n",
        "    allow_headers=['*'],\n",
        ")\n",
        "\n",
        "async def gen(model, action, prompt: str):\n",
        "    async for event in chain.astream_events(\n",
        "        {\"input\": prompt},\n",
        "        version='v1'\n",
        "    ):\n",
        "        if event['event'] == 'on_chat_model_stream':\n",
        "            yield event['data']['chunk']\n",
        "\n",
        "@app.post('/{model}/{action}/')\n",
        "async def generate(action, prompt: str = Form()):\n",
        "    return StreamingResponse(gen(model, action, prompt), media_type='text/event-stream')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfGA-P4IVUNs"
      },
      "outputs": [],
      "source": [
        "!ngrok config add-authtoken 2esCQXildH7Ma8BWGMduStFifqX_4WvZN3id9meqREwp2ApBe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3M-bbSH9D1PH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b58eca2-894a-409a-af48-1ec5e05e566b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [6967]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     85.87.86.192:0 - \"POST /llama2/generate/ HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py:1138: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
            "  warnings.warn(\n",
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [6967]\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "import ngrok\n",
        "import uvicorn\n",
        "import nest_asyncio\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "async def setup():\n",
        "        ngrok.set_auth_token(userdata.get(\"NGROK\"))\n",
        "        ngrok.forward(\n",
        "            8000,\n",
        "            authtoken_from_env=True,\n",
        "            domain=\"bright-akita-pleasantly.ngrok-free.app\"\n",
        "        )\n",
        "\n",
        "nest_asyncio.apply()\n",
        "asyncio.run(setup())\n",
        "uvicorn.run(app=app)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmhaojcY-Cxn"
      },
      "source": [
        "import deflate\n",
        "\n",
        "b64charset = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/\"\n",
        "base64_alphabet = \"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz-_\"\n",
        "source_relations = {\"Zero or One\": \"|o\", \"Exactly One\": \"||\", \"Zero or Many\": \"}o\", \"One or Many\": \"}|\"}\n",
        "target_relations = {\"Zero or One\": \"o|\", \"Exactly One\": \"||\", \"Zero or Many\": \"o{\", \"One or Many\": \"|{\"}\n",
        "\n",
        "def encode_plant_uml(diagram):\n",
        "    data = diagram.encode()\n",
        "    level = 6\n",
        "    compressed = deflate.deflate_compress(data, level)\n",
        "\n",
        "    chunks_8bit = ''.join([format(bits,'08b') for bits in compressed])\n",
        "\n",
        "    chunks_6bit = [chunks_8bit[bits:bits+6] for bits in range(0,len(chunks_8bit),6)]\n",
        "    padding_amount = ( 6 - len(chunks_6bit[len(chunks_6bit)-1]) )\n",
        "    chunks_6bit[len(chunks_6bit)-1] += padding_amount * '0'\n",
        "\n",
        "\n",
        "    encoded = ''.join([base64_alphabet[int(bits,2)] for bits in chunks_6bit])\n",
        "    encoded += int(padding_amount/2) * '='\n",
        "\n",
        "    return encoded\n",
        "\n",
        "def create_uml_diagram(data):\n",
        "    uml = '@startuml\\n'\n",
        "    for ent in data['entities']:\n",
        "        uml += 'entity ' + ent['name'] + '{\\n'\n",
        "        uml += '* ' + ent['name'] + 'ID: number <<generated>>\\n--\\n'\n",
        "        for attr in ent['attributes']:\n",
        "            if('id' in attr['name'].lower()): continue\n",
        "            uml += '* ' + attr['name'] + ': ' + attr['type'] + '\\n'\n",
        "        uml += '}\\n\\n'\n",
        "\n",
        "    for rel in data['relations']:\n",
        "        uml += rel['source'] + ' ' + source_relations[rel['cardinality_of_source']] + '--' + target_relations[rel['cardinality_of_target']] + rel['target'] + '\\n'\n",
        "\n",
        "    uml += '@enduml'\n",
        "    return encode_plant_uml(uml)\n",
        "\n",
        "def create_mermaid_diagram(data):\n",
        "    merm = 'erDiagram\\n'\n",
        "    for ent in data['entities']:\n",
        "        merm += ent['name'] + '{\\n'\n",
        "        merm += 'int ' + ent['name'] + 'ID PK\\n'\n",
        "        for attr in ent['attributes']:\n",
        "            attr['name'].replace(' ', '_')\n",
        "            if('id' in attr['name'].lower()): continue\n",
        "            merm += attr['type'] + ' ' + attr['name'] + '\\n'\n",
        "        merm += '}\\n\\n'\n",
        "\n",
        "    for rel in data['relations']:\n",
        "        merm += rel['source'] + ' ' + source_relations[rel['cardinality_of_source']] + '--' + target_relations[rel['cardinality_of_target']] + rel['target'] + ': ' + rel['name'] + '\\n'\n",
        "\n",
        "    return merm\n",
        "\n",
        "```\n",
        "# Tiene formato de código\n",
        "\n",
        "```\n",
        "# Tiene formato de código\n",
        "```\n",
        "\n",
        "\n",
        "```\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP94beoxndxx+ktu+6DZBzm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}